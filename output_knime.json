[
{"company": "Continental", "url": "https://www.knime.com/success-story/how-continental-delivered-global-analytics-upskilling-program-knime/", "title": "How Continental delivered a global analytics upskilling program with KNIME | KNIME", "content": "Open main menu How Continental delivered a global analytics upskilling program with KNIME Manufacturing Data Science Process Automation From 2 days to 30 minutes for controlling tasks Instant traceability insights for users across R&D, plant, and quality departments Since rolling out KNIME at Continental, we’re making better decisions, everywhere. We’re also able to back these decisions on stronger data, faster. We’re gaining more insights from data, and freeing human capacity from mundane data processing and reporting tasks. Dr. Arne Beckhaus Head of Big Data and Digital Transformation C&S, Continental \n Continental is committed to increasing the data literacy of the entire organization by empowering users in business functions who have no IT or programming background to work more efficiently and effectively with data and, ultimately, discover new insights with that data. Microsoft Excel used to be the go-to tool for business users handling data at Continental.  \n\n Today, it is perfectly enriched by  KNIME Analytics Platform  for standard workflows, bigger data volumes, or data discovery. For example, repetitive tasks, such as certain forecasting processes, are now automated in  KNIME Analytics Platform . Until recently, this was either done via complex spreadsheets or Visual Basic for Applications Macros (VBA, the programming language built into Excel). However, Continental began experiencing maintainability and performance issues for medium-sized data volumes. There are thousands of Excel users at Continental, all working in diverse business functions: controlling, tax, HR, R&D, production planning, etc. The need to give these users a tool which can automate mundane data processing and reporting tasks resulted in a company-wide initiative to roll out  KNIME Analytics Platform . \n\n \n Starting a global roll-out \n The roll-out of KNIME at Continental started out as a pilot project in Chassis & Safety (C&S), a €9.6bn sales division of the corporation, and was led by Dr. Arne Beckhaus - Head of Big Data and Digital Transformation C&S at Continental. The very first use case emerged in the Brake System R&D Department, where KNIME was used to generate Kanban boards based on data exported from the issue tracking system. \n\n An agile setup of starting small and expanding quickly had a positive response. It was quickly clear just how much business users loved using KNIME and how much data processing power was gained. The roll-out was made easier by positive word of mouth and a high, bottom-up demand for trainings on how to use KNIME. \n\n Many processes across the entire organization have now achieved higher levels of automation. For example, production planning, reporting (controlling, logistics, etc.), various project management tasks such as Kanban boards and deadline tracking, supply chain warnings, and so on. Data integration and data wrangling are now mostly done by business users in KNIME, and, in many cases, this step alone yields the promised data insight or process automation. Sometimes KNIME is used for an intermediate data preparation step before the data can be exported to visualization tools such as  PowerBI ,  Tableau , and MicroStrategy. Continental also uses Amazon Web Services for long-running workflows (or workflows which are RAM or CPU hungry). \n Better decisions mean huge resource savings \n Significant  time savings of over 80% in pilot projects Lead time reduction of month-end controlling tasks  from two days to thirty minutes Ability to run an entire plant budgeting process  at a tenfold precision level Serial number traceability to R&D, plant, and quality users, providing answers in minutes rather than in days And there’s a lot more potential for using KNIME throughout the organization. “One indicator for the high adoption of KNIME is the rising demand for in-house trainings” he continues. “It is great to see that we can serve this huge bottom up momentum with a very small central team”. The next steps will entail creating more user-specific training content for users in different departments, enabling a more institutionalized roll-out, and realizing and standardizing knowledge sharing across teams and departments. Due to high interest by external parties who want to learn from Continental’s data transformation journey, the decision was even made to offer data services to external clients using KNIME.  \n\n Continental Engineering Services  is an official  KNIME Partner  and able to offer these services. \n Why KNIME? \n KNIME was chosen primarily for its business user friendliness but also due to speed, data volume, breadth of functionality, and state-of-the art data wrangling and data science features. “From a business perspective, the open source model was the only commercially feasible option to rapidly spread data analytics competence throughout the organization” says Arne Beckhaus. A long tail of infrequent users would have made the license fees too expensive and a flat, company-wide license would have slowed everything down due to the initial investment. Typical commercial business models simply would not have enabled such rapid dissemination.  KNIME Server  is now also used on selected projects at Continental, enabling teams to collaborate on KNIME workflows and share amongst colleagues. \n\n KNIME is easy to maintain long term and can be enhanced with custom extensions, which are built and developed by Continental. Examples of the  Continental Extensions for KNIME  include extensions to enable custom formatting in Excel reports, or to parse third party PDF documents with text position information for automatically analyzing contract information. From an IT perspective, KNIME is easily installed on client PCs worldwide. Continental can pre-package a customized installation and deliver it as a standalone software application - making it easy for KNIME to be included in the official software distribution process. \n\n Lastly, rolling out KNIME to business departments has helped raise the awareness of analytics and artificial intelligence (AI) at the management level. They now understand the advantage of finding answers to problems by effective data wrangling and the necessity of good quality data for applying data science algorithms in KNIME. The result? Increased data literacy across the organization, more efficient, automated processes, new insights from data without blind application of AI, and better decisions everywhere. \n\n This Success Story is available  here  as a PDF. \n\n \n\n \n"},
{"company": "Centogene", "url": "https://www.knime.com/success-story/how-knime-helped-centogene-identify-biomarkers-and-improve-accuracy-patient/", "title": "How KNIME helped Centogene identify biomarkers and improve accuracy of patient diagnostics | KNIME", "content": "Open main menu How KNIME helped Centogene identify biomarkers & improve accuracy of patient diagnostics Life Sciences (Pharma & Biotech) R&D Diagnostics & Drug Discovery Faster diagnosis and treatment of rare diseases \n CENTOGENE connects patients around the world to transform the science of clinical and genetic data to diagnose, understand, and treat rare diseases. Leveraging unparalleled levels of unique data from their Bio/Databank, CENTOGENE is a leading generator of rare disease insights, who harnesses the power of multiomics and advanced technology to resolve the mysteries of rare genetic diseases. By working together with patients, physicians, and partners to research, discover, and develop medical solutions, CENTOGENE can provide answers today for a patient’s better tomorrow. For this project, the CENTOGENE team (Ionut Onila, Zhu Guanchen, Anne Schwerk) was assisted by  KNIME Partner Discngine  (Riccardo Martini). \n Access to data and algorithms for non-AI experts \n The main business problem was to push forward the frontier of metabolomics: The biomarker department was enabled to use novel and highly advanced methods for metabolomic analysis and biomarker discovery, i.e., AI-based algorithms that allow discovery of new insights on big data sets. A software environment was required in which the biomarker experts had access to the data and algorithms without having to be IT or AI experts. The environment also had to be flexible enough to enable innovation by seamlessly integrating new or improved algorithms. \n\n A collaborative and interactive infrastructure was crucial to enabling everyone to work seamlessly together. Given the nature of the work, and the speed at which CENTOGENE operates, the platform needed to be reliable, offer reproducible workflows, provide a way to standardize operations (including tracking and auditing), as well as automate certain steps. \n Strong interdepartmental collaboration with Guided Analytics \n KNIME Analytics Platform  provides the ideal environment for a team of data scientists to build several large, automated workflows and visualizations. These workflows, deployed to the  KNIME WebPortal  via  KNIME Server , create web-based applications that users can access and interact with. The intuitive and easy-to-use nature of the solution enables strong interdepartmental collaboration. With Guided Analytics, users can interact with and explore the data through interactive web pages where a KNIME workflow is running under the hood. This provides the team with access to the data, which is pre-processed and offered in a more usable form. It also provides the ability to interact with it at pre-determined points and dive deeper when needed. The seamless integration with databases, processes, and other software means KNIME is used as a hub for the optimization and linkage of a bigger software architecture. This is made possible by features such as the native  Python integration  and integration with  Jupyter notebooks . \n\n Due to this central, pivotal role, KNIME is also used as an early detection point for variations in the infrastructure to which it is connected to. This was made possible as a side benefit of a feature that was implemented primarily to increase reliability during workflow development:  automated workflow testing . Both the company and project goals intersect at the level of automation, reproducibility, and security related aspects. \n\n Security and control is addressed by native KNIME functions, such as versioning and extensive logging for auditing. However, what allows the services and implementations to exceed expectations in this case, is the flexibility with which newly developed  components  enable the sharing and reusing of KNIME workflow snippets – either in other workflows or by other teams – which ensures reproducibility and standardization of different processes \n Results \n KNIME has enabled CENTOGENE to provide their rare disease patients with better medical solutions more quickly and cost effectively. By optimizing automation, reproducibility, and security, KNIME has provided the CENTOGENE team with the tools needed to minimize time spent on non-essential training of the technology, and fewer processes with regards to how the domain expert interacts with the Bio/Databank. \n\n Using KNIME has resulted in enhanced collaboration and interactive infrastructures, which are now the core elements in creating machine learning models for biomarkers. KNIME has also enabled the automation of workflows, which has reduced the amount of manual work required by data scientists, as well as enabled users to interact with intuitive visualizations to get even more out of the data than was previously possible. Being able to meaningfully integrate data coming from different resources and experiments is what gives users the edge to address complex scientific scenarios. The overall result: the ability to identify biomarkers and improve workflows for screening and diagnosis for patients more quickly. \n Why KNIME? \n KNIME Analytics Platform  is intuitive and makes creating data science workflows easy due to  visual programming , the drag and drop workflow building method, and shallow learning curve. However, the power of KNIME lies in the self-documenting nature and reproducibility of these workflows. This ensures that knowledge and expertise is captured and saved automatically and enables others to understand what is going on. Parts of the workflow can be packaged up into  components  and shared among colleagues or added to other workflows. This guarantees standardization as well as compliance of certain steps or processes, such as data processing rules. Tracking and auditing of KNIME workflows is automatically captured via the workflow metadata. \n\n KNIME Server  plays a pivotal role in this solution because it offers workflow automation, enables collaboration among team members in remote locations, and provides users with access to the  KNIME WebPortal . It also functions as hub for the bigger infrastructure in which it is embedded. Moreover,  KNIME Server on AWS  enables users to adapt resources once the data process becomes intense, without compromising existing infrastructure and without burdening other connected resources. From a business perspective, KNIME addresses all concerns around risk, security, and auditing. Expertise from  KNIME Partner Discngine  was a strong contributing factor to the success of this project due to both their sound technical knowledge and understanding of KNIME, as well as their  life science  background. \n\n This Success Story is available  h ere  as a PDF. \n"},
{"company": "Allente", "url": "https://www.knime.com/success-story/how-allente-built-recommendation-engine-deliver-personalized-content/", "title": "How Allente built a recommendation engine to deliver personalized content | KNIME", "content": "Open main menu How Allente built a recommendation engine to deliver personalized content Media & Entertainment Marketing Marketing Analytics Providing near real-time recommendations to viewers of streaming content. \n Viewers are increasingly moving away from traditional TV to specialized media providers streaming the exact content they want. Allente offers streaming services, IPTV solutions, and fiber broadband to more than one million customers in Sweden, Norway, Denmark, and Finland.  \n\n While streaming media providers make huge amounts of content choices available to their customers, searching through the library for content they might like to watch can lead to decision fatigue and hamper the viewer experience. In the age of multiple streaming services, this can mean losing a customer to a competitor. To combat that and provide its viewers a better experience, Allente needed a way to deliver customized recommendations to help its viewers reduce the time to find the videos they would be interested in watching. \n\n \n Identifying users' viewing preferences \n Allente knew that to achieve this as fast as possible, they would need a data science platform that offered simplicity of setup and use while delivering the analytic depth to explore data, build models, and deploy them within the same platform. \n\n As an end-to-end data science platform, KNIME perfectly met these requirements. It was an easy decision for Allente to embrace KNIME as its central data science platform to deliver highly relevant, machine learning-powered recommendations to its viewers. \n\n Together with Redfield, a KNIME partner, Allente set out to build a recommendation engine using KNIME. The first step in building the recommendation engine was to connect to Allente’s main data sources in the cloud and on-premises and make sense of the troves of data they held such as event data, video-on-demand catalog, and sample tables from its legacy Oracle database. With KNIME, they built an automated data pipeline that let them connect to these varied data sources, get the data into the right shape, and categorize it into key customer segments easily in one intuitive, visual workflow environment. \n\n Once the key customer segments were defined, Redfield and Allente built and deployed the content recommendation model in the same visual environment offered by KNIME. This involved having the flexibility to code customer segments using Python, optimizing and refining them to improve the accuracy of recommendations, and finally, testing and production, all without switching tools. \n\n The model can predict content that viewers might find engaging, even for new users who do not have a viewing history. Allente is now able to deliver extremely relevant recommendations with the added upshot of improving content distribution efficiency. With KNIME, the team at Allente can easily tune and adapt the model when new consumption data surfaces, ensuring that recommendations remain relevant in the future as well. What’s more, Allente can anticipate customer behavior and predict demand such as which shows might be popular with which customer segments a couple of years from now. \n\n \n Reducing churn and improving LTV \n KNIME’s low-code capabilities have also made machine learning accessible to non-technical teams at Allente. For instance, the customer service team at Allente uses KNIME to predict the likelihood of customers churning and lowers it with targeted retention measures using behavioral insights. They also carry out customer lifetime value analysis to understand return on investment and calculate expected revenue. Additionally, they are able to improve the conversion rate of their upselling activities with personalized offers. \n\n KNIME has accelerated the adoption of a data-driven culture and data-driven decisions at Allente. One of the biggest advantages of using KNIME has been the speed and flexibility of both ETL and spinning up models.  \n\n In the words of an executive at Allente, “For us it has been key to have one tool that could do a little bit of everything. Easy to use – yet advanced and powerful. Having one hub where we can do both data-crunching, machine learning models and automated pipelines in a simple manner has transformed our life.” \n\n This Success Story is available  here  as a PDF. \n"},
{"company": "Anadolu Sigorta", "url": "https://www.knime.com/success-story/how-anadolu-sigorta-improved-their-fraud-detection-rate-51-knime/", "title": "How Anadolu Sigorta improved their fraud detection rate by 51% with KNIME | KNIME", "content": "Open main menu How Anadolu Sigorta improved their fraud detection rate by 51% with KNIME Financial Services Fraud Detection Insurance \n Anadolu Sigorta is a privately owned insurance company with offerings in non-life areas, including fire, marine, agriculture, health, and more. With 2,500 agencies in Turkey, the company plays an important role in the development of the insurance industry and in the modernization of the country’s socioeconomic environment. \n\n Backed by a strong capital base and advanced infrastructure, the company takes a pioneering approach to insurance, focused on customer satisfaction and encouraging the spread of insurance awareness throughout the country. Their unchanging principle of paying claims immediately and in full has been a source of confidence for the Turkish community. \n\n \n The growing need for better fraud detection \n Fraud is becoming more prevalent across many different industries today, taking place in insurance more than others. Due to the large number of services included in different policy coverages, detecting fraud has become more challenging (and more necessary) for Anadolu Sigorta and others in the business.    \n\n Insurance fraud can happen in two ways: a report of false or deceptive accident conditions by the policyholder themself or an organized fraudster submitting false reports using the car owner’s information (third-party offense). Anadolu Sigorta processes approximately 3,500 of these transactions a day. A transaction could be a net new claim or claim that needs reprocessing. Each claim must go through several phases or “touch points” (a very complex and time-consuming process) and might need to be processed more than once. \n\n Fraudulent claims that relate to property damage or personal injury are also extremely difficult to prove, take a lot of time and research, and could be identified as a false positive. Detecting an organized fraud ring in motor insurance is also done by manual research and can take two to three months to fully investigate.  \n\n Due to the large amount of services included in insurance coverage, the complexity of processing a claim, and trying to prove the claim, Anadolu Sigorta needed an analytics approach that could streamline, expedite, and simplify the process. \n\n \n Building a real-time, tailored fraud detection platform from the ground up \n Before KNIME, Anadolu Sigorta was using a third-party solution to detect and monitor fraud. This specific tool was costly, difficult to maintain, and couldn’t handle in-house updates or easily implement new features. Due to the flexibility of KNIME Analytics Platform, the team was able to build their own in-house fraud detection solution (and entire business process) called The SOBE Platform. \n\n The input data Anadolu Sigorta uses to analyze claims comes from the company's source systems, like their internal claim processing system, and external sources like accident report files and data from the Insurance Information and Monitoring Center (over 20 operational systems in total). In the first phase of the project, they used KNIME to collect, transform, organize, and stabilize this data.  \n\n In addition to data preparation, KNIME Analytics Platform is also used to assist with machine learning and social network analysis (SNA). Using KNIME, the data team was able to quickly build and implement two machine learning models: one to detect and predict fraudulent claim files and another to prioritize the files based on the probability of most risky to least. Prioritizing the claims by risk probability meant business units could spend less time on false positives or low risk claims and instead focus time catching the fraudulent ones.  \n\n The company also used KNIME to build a rule engine for the next step in the claim review process. The workflow includes experience-based business standards. The platform assigns the claim a score based on the results of each stage (workflow), and the results are returned to the source system via the web service configuration. Results for each claim are provided in five to 10 seconds, and they expect it to happen even faster with future integrations.  \n\n By moving data analysis and audit work to KNIME, they eliminated errors and saved time on analysis — adding value to the company. What’s more, costs related to maintenance, development, and subscriptions have decreased because they can now perform internal health checks themself without any vendor dependency. \n\n Essentially, KNIME was used to build The SOBE Platform from the ground up. They capitalized on the flexibility of KNIME Analytics Platform to build automated workflows that could process claim files instantaneously and forward them for fraud inquiry during any point in the claim review lifecycle. The SOBE Platform calculates claim file scores in real time, allowing business units to investigate and detect fraudulent activity in real time too. Beyond fraud detection, Anadolu Sigorta was also able to use KNIME to create insurance renewal probability workflows, price optimization analysis, and a customer analytics platform. \n Saving 146 million in payments from fraudulent claims \n Anadolu Sigorta can process claims and detect fraud faster and easier since implementing KNIME. They’ve achieved a 51% greater detection rate and a 54% increase in financial gain by preventing approximately 146M Turkish Liras in payments from fraudulent claims. What’s more, false positives have decreased by 31% and implementation and maintenance is done in house, costs less, and is much easier.  \n\n The SNA module significantly improved the organized fraud detection processes and reduced the effort and identification period from three to four months down to only two hours. What’s more, they were able to successfully discover eight new organized crime rings via this module. Their fraud platform is now a one-of-a-kind when it comes to configurations and structure compared to other tools on the market.  \n\n In addition to operational efficiency gains, Anadolu Sigorta was able to train business users on data science applications using KNIME. Because KNIME doesn’t require any coding, non-experts in data can create and run their own processes to solve technology issues. As an organization, Anadolu Sigorta has upskilled over 200 people in IT and other business units on how to apply data science. \n\n \n\n \n Why KNIME \n Anadolu Sigorta chose KNIME software due to its ease and speed of implementation. Building models with KNIME can be done very quickly, and their team can use existing nodes already built and available via the KNIME Hub. The team also loves KNIME for auditability and transparency of the process due to its ability to log steps while building a workflow. KNIME also integrates with different programming languages and algorithms that the team uses, like Python, R, Java Script, etc. Lastly, they like that end users can make transactions through a data app and change, alter, or define new rules to that app, if needed.  \n\n KNIME was chosen in terms of business considerations because of its simplicity. The API integration was easy to implement and the AI features for machine learning models were easily explainable to others. Both of these features allow the team to manage and trace outcomes accordingly. \n\n ⇒  Download this Innovation Note as a PDF \n\n \n\n InforA  has been Anadolu Sigorta's Solution Partner for KNIME technologies and data science since 2018. During this time, InforA's certified trainers have provided many qualified trainings on KNIME and data science to Anadolu Sigorta teams. In the last few years, these trainings have been systematically conducted as part of Anadolu Sigorta's Citizen Data Scientist Training Program. InforA's expert consultants also provide consulting support for Anadolu Sigorta's data science projects using KNIME.  \n\n To contact InforA,  click here . \n"},
{"company": "Combating Terrorism Technical Support Office", "url": "https://www.knime.com/success-story/how-combating-terrorism-office-detects-irregular-conflict/", "title": "How the Combating Terrorism Office detects irregular conflict | KNIME", "content": "Open main menu How the Combating Terrorism Office detects irregular conflict How the team leverages custom-built advanced analytics to detect gray-zone activity. Public Sector Data Science Fraud & Risk Management Icon/Play/Black Watch video 17,000 sources of news analyzed across 70 languages 15 solutions custom-built in KNIME Most projects that the CTTSO works on require out of the box solutions, which means creative thinking is needed and in many cases, the need to extend the software - which is possible with KNIME. As a result, over 15 custom nodes have been written. This makes the projects easier to manage, because they are still being completed within a single software environment. Curtis Fox Advanced Analytics Team, Combatting Terrorism Technical Support Office \n Government agencies - in this case the US government specifically - need ways to interpret things out of open-source information that are not being directly reported. This becomes useful when talking about things such as propaganda and information campaigns. These “gray-zone” activities, so termed because they straddle the watershed between political competition and conventional warfare, are difficult to detect and discern from the usual noise of legitimate socio-political discourse.  \n\n Previously, it was enough to know where the tanks, planes, and soldiers were. This was a scoped problem, which could be handled very well. With the emergence of gray-zone conflict, the aperture of what national security professionals must deal with has broadened considerably. Other emerging domains that make gray-zone activities more relevant and increasingly complex include the cyber domain and economic competition. Essentially: it’s easy to count a tank. It’s much harder to determine if something is fake news.  \n\n Furthermore, threats develop more quickly and, due to modern technology, everything is networked; a single tweet can dethrone a politician and protests can start overnight. Responding to threats must be more agile and the information must be ready to present to decision makers in a way they can digest. This necessitates a unique approach to detecting gray-zone activities from open-source data. Furthermore, attempts to leverage non-data driven approaches for detecting gray-zone activities are influenced by human bias. \n\n \n Inferring gray-zone activity \n KNIME Partner BigBear.ai  built VANE – the Virtual Anticipatory Network – which detects gray-zone activity from open-source data by reading in between the lines of directly reported phenomena. This allows decision makers to understand how US activities influence complex systems to mitigate gray-zone influences. It’s not designed to predict the precise date that an anticipated event is going to take place. VANE monitors data streams of known drivers that are predictive of the event of interest – and models courses of action to reduce the likelihood of the event’s occurrence.  \n\n VANE helps give decision makers quantitative insights to do this. It’s a data-driven platform answering questions such as what does the future hold for X? Publicly available data such as demographic data, econometric data, news, social media, web information, and more is plugged into the system and provides the insights that decision makers need to achieve the desired end-state. \n\n \n Determining signals of fake news \n VANE currently leverages 14 open-source databases, tracking 660 independent metrics. The model spans many different topic areas – from economies to weather to cyber – of which many are not pristine. For example, one of the event sources is GDELT, which tracks 17,000 different news sources in 70 different languages. However, it struggles to maintain a clean dataset. Every time an Air Jordan sneaker is sold, an event in Jordan (the country) occurs. GDELT also doesn’t determine between fake news, bad reports, and so on. Usually to compensate for low-quality data, data scientists must impute metrics or clean data-streams manually, which is time intensive and can introduce errors in the model. \n\n Therefore, tensor completion is used. Specifically matrix factorization, which is a two-degree form of tensor completion. In this case, this has been extended into multiple domains because the real world isn’t just users and ratings. It’s countries and sensors and different time dimensions, meaning there is a lot more going into the tensor. The power of tensor completion is it finds relationships not just between two entities, but between the features that are being used to model the relationship between entities. \n\n An example of how this works for image recognition: feed in some images that have pixel error, obfuscation, or where entire chunks of the image are missing entirely. The tensor completion algorithm will learn what it can about how pixels, edges, and colors relate within the image and provide a reconstructed view of what it thinks the data should look like. The other benefit is that a matrix is provided, which states where the suspected error is inside the data. \n Why KNIME? \n KNIME Analytics Platform  is great for many reasons. One of the greatest things about KNIME is that it’s a no-coding workflow building environment. However, it’s possible to add code when and where needed, which is what was done in this case. There are few developers and even less technical specialists working in the government. Therefore, giving them a platform that lowers the barrier to entry so that they can engage productively and communicate with the actual developers is important. \n\n Most projects that the CTTSO works on require out of the box solutions, which means creative thinking is needed and in many cases, the need to extend the software - which is possible with KNIME. As a result, over 15 custom nodes have been written. This makes the projects easier to manage, because they are still being completed within a single software environment. “We’ve got a lot of great things that help us take the value that KNIME gives us and fit it right in the little square peg, round hole that we’re working in. And we can do it very, very quickly, plus integrate with all the other platforms that the government likes to use” says Brian Frutchey, VANE Technical Lead at BigBear.ai. \n\n This Success Story can be downloaded  here  as a PDF. \n\n \n\n \n"},
{"company": "Alexion", "url": "https://www.knime.com/success-story/how-knime-helped-alexion-shorten-time-disease-diagnosis-accelerate-time-treatment/", "title": "How KNIME helped Alexion shorten time to disease diagnosis & accelerate time to treatment | KNIME", "content": "Open main menu How KNIME helped Alexion shorten time to disease diagnosis & accelerate time to treatment Life Sciences (Pharma & Biotech) R&D Diagnostics & Drug Discovery Diagnostic time reduced from several years to a few hours. \n People living with rare diseases often have to wait for years for an accurate diagnosis and many are never diagnosed at all. Even after diagnosis, only 5% of the 7,000 plus known rare diseases have approved therapies. Data plays a key role in diagnosing rare diseases and in the research behind their therapies – in finding insights, experimenting, and discovering new drugs. \n\n Alexion, an AstraZeneca subsidiary, is a global biopharmaceutical company that develops drugs to treat rare disorders. Its goal is to leverage the best available medical data for rare diseases and clinical insights to accelerate the diagnosis of rare conditions. \n\n \n The impact of lacking a central, trusted source of rare disease definitions \n While there is more medical data available than ever before, much of it remains untapped, disorganized, and unusable. For instance, there is no single authority providing a comprehensive list of rare disease definitions. In fact, there isn’t even a consensus on the number of rare diseases that exist. Depending on the source consulted, the answer varies between 7,000 and 9,000 rare conditions. Meanwhile, Alexion’s dataset contains over 12,000 conditions with 7,230 from Orphanet – and many more are in the process of being defined. \n\n Without a trusted and accurate source of rare disease definitions to refer to, it is difficult for physicians to correctly and quickly identify a diagnosis. Most rare diseases lead to considerable disability and early mortality, so a rapid and accurate diagnosis is crucial. Even after receiving a diagnosis, close to 90% of people with a rare disease do not have an approved treatment or therapy. The grim reality is that, of the thousands of rare conditions, only 161 have approved drugs available. The lack of a recognized inventory of rare disease definitions leads to under-representation in healthcare planning and resource allocation and prevents clinical research from being carried out. \n\n What’s more, the lack of a rare disease master dataset also has an impact on medical insurance coverage. The healthcare system in the United States uses ICD-10 codes for billing. There are barely 128 ICD-10 codes for over 7,000 rare conditions, which means that only a minuscule number of rare diseases are recognized by insurance providers and get coverage. \n\n Ultimately, a rare disease definition is the backbone to which all clinically relevant information is mapped, and a lack of it leads to delays in every aspect of disease management from drug discovery, clinical trial design, and drug approval to diagnosis, treatment plan, and insurance coverage. Alexion wanted to address these challenges by building a rare disease data master to shorten the diagnostic journey and time to initiate treatment for those living with rare conditions. \n Ingesting constantly changing medical data \n Alexion adopted KNIME as its core analytics tool to accelerate the rare disease diagnostic odyssey. With KNIME, they were able to easily ingest data from countless biomedical knowledge bases such as  Orphanet ,  PubMed ,  DrugBank ,  Reactome , and  GTex  and get it into the right shape. Before KNIME, if Orphanet or any other data source changed its data format, the team at Alexion would end up spending up to three weeks modifying their Java code, indices, and APIs to accommodate the new format. With KNIME, they were able to seamlessly integrate and transform the data in one uniform, visual environment in no time, regardless of any changes within the data source. \n\n Once the transformed data was loaded, KNIME enabled Alexion to build all-inclusive, master datasets such as the disease master, drug master, gene master, and trial master that helped close major gaps in rare disease information. \n Accurate and actionable rare disease definition dataset \n This data was sent for validation to clinicians and subject matter experts through an interactive dashboard on a KNIME-built data app. These stakeholders - typically PhDs with over 20 years of experience - often insisted on seeing the data lineage and the business logic that was followed for data mapping before they provided feedback. KNIME’s ability to automatically document each step of the data wrangling process enabled Alexion to provide stakeholders the complete visibility they needed into how data pipelines transitioned from raw data all the way to analysis. The feedback from clinicians has led to the creation of a master dataset that is 80% actionable and its quality continues to improve with time. \n Shortened rare disease diagnostic journey \n Today, this rare disease data master serves as an accurate and ready reference tool for physicians to deliver a precise diagnosis quickly. In some cases, the diagnostic time has been brought down to a few hours from several years by matching against the rare disease master. \n\n The rare disease data master also equips physicians with the information they need to provide early and effective treatment for patients after diagnosis. It reduces dependence on unreliable Google searches and addresses the clinical knowledge gap. \n\n Additionally, the rare disease data master forms the center of a flywheel of patient-centric communities around genetics, symptoms, and treatments that work, and it holds the potential to fast-track discovery and approval of new therapies. \n Improved rare disease recognition and medical coverage \n A single, trusted inventory of rare disease data bolsters the chances of higher rare disease representation in healthcare coding systems such as ICD. This would lead to increased interest in clinical research from the healthcare community as well as the much-needed recognition of rare conditions by insurance providers. \n Why KNIME? \n KNIME’s open-source nature was a key factor in it becoming the tool of choice for Alexion. Multiple research institutes that Alexion partners with, were able to start using KNIME without any barriers on scaling. The ability to share complex data with clinicians and subject matter experts through an easy-to-use, interactive data app coupled with the ability to track data lineage was invaluable for Alexion.With KNIME, Alexion and its partners were able to work on the same analytical pipeline while executing it independently on their own datasets. In other words, it helped them bring the analytics to the data and not the other way round. \n\n This Success Story is available  here  as a PDF. \n"},
{"company": "BGIS", "url": "https://www.knime.com/success-story/how-bgis-saved-400k-annually-automating-work-order-parsing-knime/", "title": "How BGIS saved $400k annually by automating work order parsing with KNIME | KNIME", "content": "Open main menu How BGIS saved $400k annually by automating work order parsing with KNIME Business Services Finance FP&A $420K saved on maintenance and repair costs $35K saved monthly in work order costs \"KNIME has generated a new-found excitement and change in approach towards self-service ETL and data science. This has resulted in significant time savings, improved auditability, and centralization of information via other projects. Workflows are extremely easy for business users to develop in KNIME – making it possible to turnaround client analysis requests in shorter amounts of time, and at levels of granularity and accuracy simply not possible with conventional office software.” Paras Gupta Director, BI & Advanced Analytics \n BGIS manages large capital projects for its clients, some involving a targeted optimization of building systems and equipment to drive efficiency, which aligns well with the company’s sustainability focus. This case focuses on a lighting retrofit conducted in hundreds of a client’s retail sites. Post-retrofit, the client wanted to assess whether the value delivered had been accurately calculated, compare the results with the original business case, and use the outcome of the investigation to inform further retrofit decisions. \n\n Clients accrue several benefits through lighting retrofits – including reductions in electricity consumption, GHG emissions and work order service calls. While electricity and GHG emission reductions are relatively easy to measure or calculate through meter bills, quantifying savings delivered as a result of work order reduction is complicated for several reasons. In this case, work orders represent events when a technician attends a building site to service or replace affected lights. This information consists of numerical as well as text data. To make things more complicated, a lighting retrofit isn’t always about changing to a completely new light bulb. It could be upgrading to a newer model of the same type of light bulb, which was the case here: upgrading from fluorescent tube light type A1 to A2. This is a level of detail simply not categorized in an easy-to-analyze fashion, yet the information is hidden in plain sight - in the text. \n Eliminating the need to manually read through 30,000 work order descriptions \n In order to understand this information, the traditional approach would have been for someone to manually read through work order problem descriptions – 30,000 to be precise – and subjectively categorize them. Furthermore, not all sites participated in the retrofit project, and as the project spanned multiple years, sites were not all retrofit at the same time. The typical approach of analyzing such a large volume of data would be to aggregate information at the client level, which would result in any savings benefits at test sites being diluted by control sites. \n\n In a nutshell: while the data existed in the database, extracting information from large quantities of work orders, and the corresponding textual fields was complex. It simply wasn’t as easy to prove retrofit savings as it may have appeared. The client was relying on a detailed analysis to help inform future lighting benefits, so the analysis had to be spot on to ensure a correct decision. \n Using data science to deep dive into topics and ensure accuracy \n The first step in defining an objective and efficient way to quantify savings was to identify a baseline: when did the retrofit happen, and which sites were retrofit? Looking at the costs on either side of the baseline for test sites, for both retrofit sites and non-retrofit sites, it’s possible to identify the impact of the retrofit. \n\n Proving that the drop was in fact driven by the fluorescent tube light change from type A1 to A2 was the next challenge - as that level of detail is not kept as a tabular record and is difficult to extract. The solution for this was through application of data science. Topic modeling, an unsupervised natural language processing (NLP) technique, was used to read through all work orders’ descriptions and resolutions - to understand what issue occurred, and what work was performed at the site. This technique categorized the service calls in an objective fashion, providing statistics to deep dive into the topics to ensure accuracy. In one case, topic modelling detected a category of work orders (“ceil, height, standard, fluoresce”) where service calls had been initiated to change a fluorescent tube light at ceiling height. This was clearly an activity which was within the scope of the retrofit project objective i.e. the project aimed to reduce such types of work orders. Several other in-scope themes of service calls were also identified which occurred prior to the retrofit. \n\n Topic modelling was conducted in both the pre-retrofit and post-retrofit phases to identify (1) the types and counts of work orders created, (2) whether the underlying issues were those which the retrofit was designed to address, and (3) the associated costs within each type of topic. \n\n The issues that the retrofit was designed to address – like in the example above – reduced dramatically in quantity and in the overall priority post-retrofit. When compared to non-retrofit sites, it became further apparent that the retrofit resulted in cost reductions. Topic modelling allowed BGIS to attribute these savings to the particular type of light bulb that was replaced under the retrofit. \n Results \n At a high level, the lighting retrofit project was budgeted at approximately $4M. The savings were a combination of reductions in both electricity consumption and maintenance and repair costs (M & R work orders). Energy savings were measurable from the bills, while the M & R savings required the topic modeling approach. Therefore the savings just on M & R work orders of $420K (annualized) is substantial as a proportion of the overall project budget and helped to justify the overall project costs. Specific results were: \n\n $420K worth of annual cost savings $35K reduction in average monthly work order costs Potential for even more savings As a service provider, this analysis demonstrated that savings could potentially have been higher had all sites been retrofit – the sites which did not go through the retrofit in fact increased in cost over time over the baseline. Additionally, future retrofits may be condensed over shorter time frames to reap cost avoidance benefits, leading to stronger business cases and savings for clients. From an analysis perspective, modern technology can quickly address common business problems in an objective, transparent, and repeatable fashion. A risk with analyses conducted at aggregate levels is that such analyses can easily provide an incorrect direction (Simpson’s Paradox); in this case, had an aggregated method been used, it could have incorrectly led to an understatement of the savings delivered for the client. \n Why KNIME? \n A review of independent research papers identified KNIME as a Leader in Gartner’s 2018 Magic Quadrant for Data Science and Machine Learning Platforms: a position KNIME had retained for four years prior. Additionally, the total cost of ownership for KNIME was dramatically lower than other software providers in the same quadrant. \n\n As brand new tool to the organization,  KNIME Analytics Platform  was simple to learn thanks to an extensive example library ( KNIME Hub ), several  free and paid online courses , a buzzing online community on  KNIME Forum  (as is typical with many open source tools), and a responsive support team. A key reason for selecting KNIME was the no-fee,  one-click download . Other data science tools were considered, but the high licensing fees quickly made the total cost of ownership unpalatable. Also, because KNIME doesn’t work in competition with existing tools, but rather alongside them, it provided peace of mind that the tools the business is familiar with could still be used – including  Access ,  SQL  and  Tableau . \n\n Getting started with KNIME  was also very easy – thanks to all the free resources available online. Paras Gupta, Director, BI & Advanced Analytics at BGIS went from  \"having zero experience to being an advanced user in under two weeks.” \n\n Furthermore, in this case specifically, the client was able to go back and justify further business cases - helping BGIS to prove value and to continue delivering value to clients. \n\n This Success Story is available  here  as a PDF. \n"}
][
{"company": "Chiesi", "url": "https://www.knime.com/success-story/how-chiesi-automated-physico-chemical-property-calculation-50000-compounds/", "title": "How Chiesi automated the physico-chemical property calculation of 50,000 + compounds | KNIME", "content": "Open main menu How Chiesi automated the physico-chemical property calculation of 50,000 + compounds Life Sciences (Pharma & Biotech) R&D Diagnostics & Drug Discovery Significant time savings for scientists, who no longer need to calculate properties on demand, resulting in increased customer satisfaction. \n \n Getting information to medicinal chemists in a timely manner \n The development of drug candidates that combine an acceptable biological activity and an appropriate physico-chemical profile is a key challenge. Therefore, in the drug discovery process, physico-chemical properties are important parameters for the characterization of compounds. In the search for new drug candidates, medicinal chemists routinely evaluate data such as biological activities and physico-chemical properties associated to numerous compounds. This is to prioritize the most promising ones for further optimization or study and discard the others. The present workflow helps chemists evaluate whether the compounds possess desirable physico-chemical properties such as solubility, pKa, and Lipinski criteria. \n\n The goal of this project was to provide essential information to all medicinal chemists in a timely manner. All scientists, about one hundred people – and not only MedChem or computational chemists - all benefit from these calculated properties at various stages of the discovery process. \n\n The specific requirements of this project included: \n\n Integrate physico-chemical properties (calculated by ACD/Labs Percepta) to the corporate database. Implement a user-friendly data visualization format for pKa values to simplify association of calculated property to compound functional group. Automate the entire process without any human intervention (e-mail if workflow ends with success). Run the workflow once a day, during night. Extract newly registered (i.e. without calculated physico-chemical properties) molecules from dedicated ORACLE™ view. Interact with ACD/Labs Percepta in order to calculate physico-chemical properties. Parse the calculated results and upload to the corporate database. Render the annotated structures in PNG image and upload to the corporate database. Fully automating the evaluation of drug compounds To aid chemists in evaluating whether compounds have desirable physico-chemical properties, a KNIME workflow was developed that that routinely updates compounds registered in the proprietary database, with the corresponding predicted physico-chemical properties (LogP, LogD, LogS and pKa). A commercial program for property calculation (ACD/Labs Percepta) has been coupled with KNIME Analytics Platform and KNIME Server to fully automate this procedure for all new chemical entities registered in the company database. The KNIME workflow, deployed on KNIME Server, is executed automatically at a given time, and results are stored in the Chiesi proprietary corporate DB. \n\n The project started with verifying that ACD/Labs Percepta (batch module) could calculate all the needed properties via command-line and that the results were compatible with standard KNIME nodes – specifically SDF Reader and CSV Reader. A set of molecule structures was then received from a public structure database to use in setting up a properties calculation different enough to cover most calculation problems. Then the structure’s format of the input table coming from the ORACLE™ view was defined (i.e. SDF or SMILES format of molecules, identifier, primary keys, other fields), as well as the output format to write to ORACLE™ tables (table names, fields name and type, accessory columns). \n\n The construction of the workflow looked like this: \n\n Build the ORACLE™ table similar to production environment, for the read and write/update. Export SMILES structures, transform and write an SDF file, and instruct external applications to read the last one using the External Tool node. Gather the output files (CSV and SDF). Split data between a single-type property (LogP, LogD and LogS at pH 7.4) to be registered in the molecule table, multiple-type property (pKa values) to be registered in its proper table and warning message to be archived as logfile.txt – all using the CSV Reader Output. Annotate the pKa values in the SDF structure, render as PNG image and load as binary object in a third table – all using SDF Reader Output. Send email to administrator (assuming all is fine) automatically using KNIME Server. \n\n \n Results \n This project has resulted in approximately 50,000 compounds with calculated properties over a timeframe of more than 4 years - without any problem or intervention. The biggest impact that this solution has had, is the time saved by scientists who no longer need to calculate properties on demand – as a result, customer satisfaction has also increased considerably. The biggest lesson learned: solving a real-life business case using integration and automation increase productivity and user experience. \n Why KNIME? \n Before the project began, two key features were required. The first being the ability to interact (read, write and update permissions) with ORACLE™ Database. The second: the ability to interact via command line with third party software (ACD/Labs Percepta Batch). KNIME Analytics Platform enabled us to do these two things. \n\n To start with, the free and open source KNIME Analytics Platform played an important role – largely due to the significant cost advantages that an open source software has, as well as the number of internal KNIME advocates who had already been using KNIME . Once acceptance for KNIME Analytics Platform grew, getting a license for KNIME Server was much simpler. Furthermore, adoption of KNIME Server was driven by the possibility to solve other use cases across different departments. \n\n This Success Story can be downloaded here as a PDF . \n"},
{"company": "Anadolu Sigorta", "url": "https://www.knime.com/success-story/how-anadolu-sigorta-improved-their-fraud-detection-rate-51-knime/", "title": "How Anadolu Sigorta improved their fraud detection rate by 51% with KNIME | KNIME", "content": "Open main menu How Anadolu Sigorta improved their fraud detection rate by 51% with KNIME Financial Services Fraud Detection Insurance \n Anadolu Sigorta is a privately owned insurance company with offerings in non-life areas, including fire, marine, agriculture, health, and more. With 2,500 agencies in Turkey, the company plays an important role in the development of the insurance industry and in the modernization of the country’s socioeconomic environment. \n\n Backed by a strong capital base and advanced infrastructure, the company takes a pioneering approach to insurance, focused on customer satisfaction and encouraging the spread of insurance awareness throughout the country. Their unchanging principle of paying claims immediately and in full has been a source of confidence for the Turkish community. \n\n \n The growing need for better fraud detection \n Fraud is becoming more prevalent across many different industries today, taking place in insurance more than others. Due to the large number of services included in different policy coverages, detecting fraud has become more challenging (and more necessary) for Anadolu Sigorta and others in the business.    \n\n Insurance fraud can happen in two ways: a report of false or deceptive accident conditions by the policyholder themself or an organized fraudster submitting false reports using the car owner’s information (third-party offense). Anadolu Sigorta processes approximately 3,500 of these transactions a day. A transaction could be a net new claim or claim that needs reprocessing. Each claim must go through several phases or “touch points” (a very complex and time-consuming process) and might need to be processed more than once. \n\n Fraudulent claims that relate to property damage or personal injury are also extremely difficult to prove, take a lot of time and research, and could be identified as a false positive. Detecting an organized fraud ring in motor insurance is also done by manual research and can take two to three months to fully investigate.  \n\n Due to the large amount of services included in insurance coverage, the complexity of processing a claim, and trying to prove the claim, Anadolu Sigorta needed an analytics approach that could streamline, expedite, and simplify the process. \n\n \n Building a real-time, tailored fraud detection platform from the ground up \n Before KNIME, Anadolu Sigorta was using a third-party solution to detect and monitor fraud. This specific tool was costly, difficult to maintain, and couldn’t handle in-house updates or easily implement new features. Due to the flexibility of KNIME Analytics Platform, the team was able to build their own in-house fraud detection solution (and entire business process) called The SOBE Platform. \n\n The input data Anadolu Sigorta uses to analyze claims comes from the company's source systems, like their internal claim processing system, and external sources like accident report files and data from the Insurance Information and Monitoring Center (over 20 operational systems in total). In the first phase of the project, they used KNIME to collect, transform, organize, and stabilize this data.  \n\n In addition to data preparation, KNIME Analytics Platform is also used to assist with machine learning and social network analysis (SNA). Using KNIME, the data team was able to quickly build and implement two machine learning models: one to detect and predict fraudulent claim files and another to prioritize the files based on the probability of most risky to least. Prioritizing the claims by risk probability meant business units could spend less time on false positives or low risk claims and instead focus time catching the fraudulent ones.  \n\n The company also used KNIME to build a rule engine for the next step in the claim review process. The workflow includes experience-based business standards. The platform assigns the claim a score based on the results of each stage (workflow), and the results are returned to the source system via the web service configuration. Results for each claim are provided in five to 10 seconds, and they expect it to happen even faster with future integrations.  \n\n By moving data analysis and audit work to KNIME, they eliminated errors and saved time on analysis — adding value to the company. What’s more, costs related to maintenance, development, and subscriptions have decreased because they can now perform internal health checks themself without any vendor dependency. \n\n Essentially, KNIME was used to build The SOBE Platform from the ground up. They capitalized on the flexibility of KNIME Analytics Platform to build automated workflows that could process claim files instantaneously and forward them for fraud inquiry during any point in the claim review lifecycle. The SOBE Platform calculates claim file scores in real time, allowing business units to investigate and detect fraudulent activity in real time too. Beyond fraud detection, Anadolu Sigorta was also able to use KNIME to create insurance renewal probability workflows, price optimization analysis, and a customer analytics platform. \n Saving 146 million in payments from fraudulent claims \n Anadolu Sigorta can process claims and detect fraud faster and easier since implementing KNIME. They’ve achieved a 51% greater detection rate and a 54% increase in financial gain by preventing approximately 146M Turkish Liras in payments from fraudulent claims. What’s more, false positives have decreased by 31% and implementation and maintenance is done in house, costs less, and is much easier.  \n\n The SNA module significantly improved the organized fraud detection processes and reduced the effort and identification period from three to four months down to only two hours. What’s more, they were able to successfully discover eight new organized crime rings via this module. Their fraud platform is now a one-of-a-kind when it comes to configurations and structure compared to other tools on the market.  \n\n In addition to operational efficiency gains, Anadolu Sigorta was able to train business users on data science applications using KNIME. Because KNIME doesn’t require any coding, non-experts in data can create and run their own processes to solve technology issues. As an organization, Anadolu Sigorta has upskilled over 200 people in IT and other business units on how to apply data science. \n\n \n\n \n Why KNIME \n Anadolu Sigorta chose KNIME software due to its ease and speed of implementation. Building models with KNIME can be done very quickly, and their team can use existing nodes already built and available via the KNIME Hub. The team also loves KNIME for auditability and transparency of the process due to its ability to log steps while building a workflow. KNIME also integrates with different programming languages and algorithms that the team uses, like Python, R, Java Script, etc. Lastly, they like that end users can make transactions through a data app and change, alter, or define new rules to that app, if needed.  \n\n KNIME was chosen in terms of business considerations because of its simplicity. The API integration was easy to implement and the AI features for machine learning models were easily explainable to others. Both of these features allow the team to manage and trace outcomes accordingly. \n\n ⇒  Download this Innovation Note as a PDF \n\n \n\n InforA  has been Anadolu Sigorta's Solution Partner for KNIME technologies and data science since 2018. During this time, InforA's certified trainers have provided many qualified trainings on KNIME and data science to Anadolu Sigorta teams. In the last few years, these trainings have been systematically conducted as part of Anadolu Sigorta's Citizen Data Scientist Training Program. InforA's expert consultants also provide consulting support for Anadolu Sigorta's data science projects using KNIME.  \n\n To contact InforA,  click here . \n"},
{"company": "ClearPeaks", "url": "https://www.knime.com/success-story/how-knime-helps-hr-teams-predict-attrition-low-code-machine-learning/", "title": "How KNIME helps HR teams predict attrition with low-code machine learning | KNIME", "content": "Open main menu How KNIME helps HR teams predict attrition with low-code machine learning Business Services Human Resources HR Analytics \n \n The challenge: Use attrition analysis to design effective talent retention strategies \n Employee attrition is a significant cost to an organization. A high attrition rate can lead to increased tangible costs such as training, recruitment, and on-boarding, as well as intangible costs such as project management and customer relationships. Attrition analysis and support in defining an optimal talent retention strategy. However, this requires a deep understanding of employee behavior. If HR directors designed talent retention strategies based on data and machine learning, companies could significantly reduce attrition and see an increase in employee productivity and company profitability. Together with attrition, the quantification of employee value and attrition cost is key to making optimal HR decisions. \n The solution: A KNIME Guided Analytics application \n A KNIME workflow collects employee data provided by the HR department and estimates the probability of each employee leaving the company. The dataset is then cleaned up for outliers, erroneous values, and/or representational structure. Employee attrition datasets are usually imbalanced for the attrition category; hence a rebalancing exercise is performed. Several models such as Random Forest, Logistic Regression, Naïve Bayes, and Gradient Boosted are trained, with the best performing model being chosen to score current employees. The workflow is deployed on  KNIME Server  and can be executed on demand or using the scheduling option, to update attrition probabilities. A dataset is generated with the model output, which the HR department uses for analysis on the profiles of the employees likely to leave. \n\n With the native  Tableau Integration  in  KNIME Analytics Platform , the results are exported directly to a Tableau dashboard. This provides business users with access to informative and useful visualizations quickly and easily. A  Guided Analytics  application, made accessible via the KNIME WebPortal, enables business users to make parametric changes to understand their impact on the attrition process. Business users can also simulate scenarios, which empowers them and promotes a data-driven decision-making culture. \n\n Both workflows are available for download on  KNIME Hub : \n\n Training a Churn Predictor Workflow Deploying a Churn Predictor Workflow Why KNIME? KNIME provides the ideal environment for building a classification machine learning model, and conducting an attrition analysis. It’s possible to build workflows using the appropriate connector for different data sources, transform data, and train machine learning models.  KNIME Server  enables these workflows to be executed at predefined times. The  KNIME WebPortal  allows business users to interact with these workflows by uploading their own data or updating the attrition probabilities on demand. The seamless integration with  Tableau , enables users to create dashboards in order to dive deeper into the results. \n\n This Innovation Note is available  here  as a PDF. \n"},
{"company": "Continental", "url": "https://www.knime.com/success-story/how-continental-delivered-global-analytics-upskilling-program-knime/", "title": "How Continental delivered a global analytics upskilling program with KNIME | KNIME", "content": "Open main menu How Continental delivered a global analytics upskilling program with KNIME Manufacturing Data Science Process Automation From 2 days to 30 minutes for controlling tasks Instant traceability insights for users across R&D, plant, and quality departments Since rolling out KNIME at Continental, we’re making better decisions, everywhere. We’re also able to back these decisions on stronger data, faster. We’re gaining more insights from data, and freeing human capacity from mundane data processing and reporting tasks. Dr. Arne Beckhaus Head of Big Data and Digital Transformation C&S, Continental \n Continental is committed to increasing the data literacy of the entire organization by empowering users in business functions who have no IT or programming background to work more efficiently and effectively with data and, ultimately, discover new insights with that data. Microsoft Excel used to be the go-to tool for business users handling data at Continental.  \n\n Today, it is perfectly enriched by  KNIME Analytics Platform  for standard workflows, bigger data volumes, or data discovery. For example, repetitive tasks, such as certain forecasting processes, are now automated in  KNIME Analytics Platform . Until recently, this was either done via complex spreadsheets or Visual Basic for Applications Macros (VBA, the programming language built into Excel). However, Continental began experiencing maintainability and performance issues for medium-sized data volumes. There are thousands of Excel users at Continental, all working in diverse business functions: controlling, tax, HR, R&D, production planning, etc. The need to give these users a tool which can automate mundane data processing and reporting tasks resulted in a company-wide initiative to roll out  KNIME Analytics Platform . \n\n \n Starting a global roll-out \n The roll-out of KNIME at Continental started out as a pilot project in Chassis & Safety (C&S), a €9.6bn sales division of the corporation, and was led by Dr. Arne Beckhaus - Head of Big Data and Digital Transformation C&S at Continental. The very first use case emerged in the Brake System R&D Department, where KNIME was used to generate Kanban boards based on data exported from the issue tracking system. \n\n An agile setup of starting small and expanding quickly had a positive response. It was quickly clear just how much business users loved using KNIME and how much data processing power was gained. The roll-out was made easier by positive word of mouth and a high, bottom-up demand for trainings on how to use KNIME. \n\n Many processes across the entire organization have now achieved higher levels of automation. For example, production planning, reporting (controlling, logistics, etc.), various project management tasks such as Kanban boards and deadline tracking, supply chain warnings, and so on. Data integration and data wrangling are now mostly done by business users in KNIME, and, in many cases, this step alone yields the promised data insight or process automation. Sometimes KNIME is used for an intermediate data preparation step before the data can be exported to visualization tools such as  PowerBI ,  Tableau , and MicroStrategy. Continental also uses Amazon Web Services for long-running workflows (or workflows which are RAM or CPU hungry). \n Better decisions mean huge resource savings \n Significant  time savings of over 80% in pilot projects Lead time reduction of month-end controlling tasks  from two days to thirty minutes Ability to run an entire plant budgeting process  at a tenfold precision level Serial number traceability to R&D, plant, and quality users, providing answers in minutes rather than in days And there’s a lot more potential for using KNIME throughout the organization. “One indicator for the high adoption of KNIME is the rising demand for in-house trainings” he continues. “It is great to see that we can serve this huge bottom up momentum with a very small central team”. The next steps will entail creating more user-specific training content for users in different departments, enabling a more institutionalized roll-out, and realizing and standardizing knowledge sharing across teams and departments. Due to high interest by external parties who want to learn from Continental’s data transformation journey, the decision was even made to offer data services to external clients using KNIME.  \n\n Continental Engineering Services  is an official  KNIME Partner  and able to offer these services. \n Why KNIME? \n KNIME was chosen primarily for its business user friendliness but also due to speed, data volume, breadth of functionality, and state-of-the art data wrangling and data science features. “From a business perspective, the open source model was the only commercially feasible option to rapidly spread data analytics competence throughout the organization” says Arne Beckhaus. A long tail of infrequent users would have made the license fees too expensive and a flat, company-wide license would have slowed everything down due to the initial investment. Typical commercial business models simply would not have enabled such rapid dissemination.  KNIME Server  is now also used on selected projects at Continental, enabling teams to collaborate on KNIME workflows and share amongst colleagues. \n\n KNIME is easy to maintain long term and can be enhanced with custom extensions, which are built and developed by Continental. Examples of the  Continental Extensions for KNIME  include extensions to enable custom formatting in Excel reports, or to parse third party PDF documents with text position information for automatically analyzing contract information. From an IT perspective, KNIME is easily installed on client PCs worldwide. Continental can pre-package a customized installation and deliver it as a standalone software application - making it easy for KNIME to be included in the official software distribution process. \n\n Lastly, rolling out KNIME to business departments has helped raise the awareness of analytics and artificial intelligence (AI) at the management level. They now understand the advantage of finding answers to problems by effective data wrangling and the necessity of good quality data for applying data science algorithms in KNIME. The result? Increased data literacy across the organization, more efficient, automated processes, new insights from data without blind application of AI, and better decisions everywhere. \n\n This Success Story is available  here  as a PDF. \n\n \n\n \n"},
{"company": "Centogene", "url": "https://www.knime.com/success-story/how-knime-helped-centogene-identify-biomarkers-and-improve-accuracy-patient/", "title": "How KNIME helped Centogene identify biomarkers and improve accuracy of patient diagnostics | KNIME", "content": "Open main menu How KNIME helped Centogene identify biomarkers & improve accuracy of patient diagnostics Life Sciences (Pharma & Biotech) R&D Diagnostics & Drug Discovery Faster diagnosis and treatment of rare diseases \n CENTOGENE connects patients around the world to transform the science of clinical and genetic data to diagnose, understand, and treat rare diseases. Leveraging unparalleled levels of unique data from their Bio/Databank, CENTOGENE is a leading generator of rare disease insights, who harnesses the power of multiomics and advanced technology to resolve the mysteries of rare genetic diseases. By working together with patients, physicians, and partners to research, discover, and develop medical solutions, CENTOGENE can provide answers today for a patient’s better tomorrow. For this project, the CENTOGENE team (Ionut Onila, Zhu Guanchen, Anne Schwerk) was assisted by  KNIME Partner Discngine  (Riccardo Martini). \n Access to data and algorithms for non-AI experts \n The main business problem was to push forward the frontier of metabolomics: The biomarker department was enabled to use novel and highly advanced methods for metabolomic analysis and biomarker discovery, i.e., AI-based algorithms that allow discovery of new insights on big data sets. A software environment was required in which the biomarker experts had access to the data and algorithms without having to be IT or AI experts. The environment also had to be flexible enough to enable innovation by seamlessly integrating new or improved algorithms. \n\n A collaborative and interactive infrastructure was crucial to enabling everyone to work seamlessly together. Given the nature of the work, and the speed at which CENTOGENE operates, the platform needed to be reliable, offer reproducible workflows, provide a way to standardize operations (including tracking and auditing), as well as automate certain steps. \n Strong interdepartmental collaboration with Guided Analytics \n KNIME Analytics Platform  provides the ideal environment for a team of data scientists to build several large, automated workflows and visualizations. These workflows, deployed to the  KNIME WebPortal  via  KNIME Server , create web-based applications that users can access and interact with. The intuitive and easy-to-use nature of the solution enables strong interdepartmental collaboration. With Guided Analytics, users can interact with and explore the data through interactive web pages where a KNIME workflow is running under the hood. This provides the team with access to the data, which is pre-processed and offered in a more usable form. It also provides the ability to interact with it at pre-determined points and dive deeper when needed. The seamless integration with databases, processes, and other software means KNIME is used as a hub for the optimization and linkage of a bigger software architecture. This is made possible by features such as the native  Python integration  and integration with  Jupyter notebooks . \n\n Due to this central, pivotal role, KNIME is also used as an early detection point for variations in the infrastructure to which it is connected to. This was made possible as a side benefit of a feature that was implemented primarily to increase reliability during workflow development:  automated workflow testing . Both the company and project goals intersect at the level of automation, reproducibility, and security related aspects. \n\n Security and control is addressed by native KNIME functions, such as versioning and extensive logging for auditing. However, what allows the services and implementations to exceed expectations in this case, is the flexibility with which newly developed  components  enable the sharing and reusing of KNIME workflow snippets – either in other workflows or by other teams – which ensures reproducibility and standardization of different processes \n Results \n KNIME has enabled CENTOGENE to provide their rare disease patients with better medical solutions more quickly and cost effectively. By optimizing automation, reproducibility, and security, KNIME has provided the CENTOGENE team with the tools needed to minimize time spent on non-essential training of the technology, and fewer processes with regards to how the domain expert interacts with the Bio/Databank. \n\n Using KNIME has resulted in enhanced collaboration and interactive infrastructures, which are now the core elements in creating machine learning models for biomarkers. KNIME has also enabled the automation of workflows, which has reduced the amount of manual work required by data scientists, as well as enabled users to interact with intuitive visualizations to get even more out of the data than was previously possible. Being able to meaningfully integrate data coming from different resources and experiments is what gives users the edge to address complex scientific scenarios. The overall result: the ability to identify biomarkers and improve workflows for screening and diagnosis for patients more quickly. \n Why KNIME? \n KNIME Analytics Platform  is intuitive and makes creating data science workflows easy due to  visual programming , the drag and drop workflow building method, and shallow learning curve. However, the power of KNIME lies in the self-documenting nature and reproducibility of these workflows. This ensures that knowledge and expertise is captured and saved automatically and enables others to understand what is going on. Parts of the workflow can be packaged up into  components  and shared among colleagues or added to other workflows. This guarantees standardization as well as compliance of certain steps or processes, such as data processing rules. Tracking and auditing of KNIME workflows is automatically captured via the workflow metadata. \n\n KNIME Server  plays a pivotal role in this solution because it offers workflow automation, enables collaboration among team members in remote locations, and provides users with access to the  KNIME WebPortal . It also functions as hub for the bigger infrastructure in which it is embedded. Moreover,  KNIME Server on AWS  enables users to adapt resources once the data process becomes intense, without compromising existing infrastructure and without burdening other connected resources. From a business perspective, KNIME addresses all concerns around risk, security, and auditing. Expertise from  KNIME Partner Discngine  was a strong contributing factor to the success of this project due to both their sound technical knowledge and understanding of KNIME, as well as their  life science  background. \n\n This Success Story is available  h ere  as a PDF. \n"},
{"company": "Combating Terrorism Technical Support Office", "url": "https://www.knime.com/success-story/how-combating-terrorism-office-detects-irregular-conflict/", "title": "How the Combating Terrorism Office detects irregular conflict | KNIME", "content": "Open main menu How the Combating Terrorism Office detects irregular conflict How the team leverages custom-built advanced analytics to detect gray-zone activity. Public Sector Data Science Fraud & Risk Management Icon/Play/Black Watch video 17,000 sources of news analyzed across 70 languages 15 solutions custom-built in KNIME Most projects that the CTTSO works on require out of the box solutions, which means creative thinking is needed and in many cases, the need to extend the software - which is possible with KNIME. As a result, over 15 custom nodes have been written. This makes the projects easier to manage, because they are still being completed within a single software environment. Curtis Fox Advanced Analytics Team, Combatting Terrorism Technical Support Office \n Government agencies - in this case the US government specifically - need ways to interpret things out of open-source information that are not being directly reported. This becomes useful when talking about things such as propaganda and information campaigns. These “gray-zone” activities, so termed because they straddle the watershed between political competition and conventional warfare, are difficult to detect and discern from the usual noise of legitimate socio-political discourse.  \n\n Previously, it was enough to know where the tanks, planes, and soldiers were. This was a scoped problem, which could be handled very well. With the emergence of gray-zone conflict, the aperture of what national security professionals must deal with has broadened considerably. Other emerging domains that make gray-zone activities more relevant and increasingly complex include the cyber domain and economic competition. Essentially: it’s easy to count a tank. It’s much harder to determine if something is fake news.  \n\n Furthermore, threats develop more quickly and, due to modern technology, everything is networked; a single tweet can dethrone a politician and protests can start overnight. Responding to threats must be more agile and the information must be ready to present to decision makers in a way they can digest. This necessitates a unique approach to detecting gray-zone activities from open-source data. Furthermore, attempts to leverage non-data driven approaches for detecting gray-zone activities are influenced by human bias. \n\n \n Inferring gray-zone activity \n KNIME Partner BigBear.ai  built VANE – the Virtual Anticipatory Network – which detects gray-zone activity from open-source data by reading in between the lines of directly reported phenomena. This allows decision makers to understand how US activities influence complex systems to mitigate gray-zone influences. It’s not designed to predict the precise date that an anticipated event is going to take place. VANE monitors data streams of known drivers that are predictive of the event of interest – and models courses of action to reduce the likelihood of the event’s occurrence.  \n\n VANE helps give decision makers quantitative insights to do this. It’s a data-driven platform answering questions such as what does the future hold for X? Publicly available data such as demographic data, econometric data, news, social media, web information, and more is plugged into the system and provides the insights that decision makers need to achieve the desired end-state. \n\n \n Determining signals of fake news \n VANE currently leverages 14 open-source databases, tracking 660 independent metrics. The model spans many different topic areas – from economies to weather to cyber – of which many are not pristine. For example, one of the event sources is GDELT, which tracks 17,000 different news sources in 70 different languages. However, it struggles to maintain a clean dataset. Every time an Air Jordan sneaker is sold, an event in Jordan (the country) occurs. GDELT also doesn’t determine between fake news, bad reports, and so on. Usually to compensate for low-quality data, data scientists must impute metrics or clean data-streams manually, which is time intensive and can introduce errors in the model. \n\n Therefore, tensor completion is used. Specifically matrix factorization, which is a two-degree form of tensor completion. In this case, this has been extended into multiple domains because the real world isn’t just users and ratings. It’s countries and sensors and different time dimensions, meaning there is a lot more going into the tensor. The power of tensor completion is it finds relationships not just between two entities, but between the features that are being used to model the relationship between entities. \n\n An example of how this works for image recognition: feed in some images that have pixel error, obfuscation, or where entire chunks of the image are missing entirely. The tensor completion algorithm will learn what it can about how pixels, edges, and colors relate within the image and provide a reconstructed view of what it thinks the data should look like. The other benefit is that a matrix is provided, which states where the suspected error is inside the data. \n Why KNIME? \n KNIME Analytics Platform  is great for many reasons. One of the greatest things about KNIME is that it’s a no-coding workflow building environment. However, it’s possible to add code when and where needed, which is what was done in this case. There are few developers and even less technical specialists working in the government. Therefore, giving them a platform that lowers the barrier to entry so that they can engage productively and communicate with the actual developers is important. \n\n Most projects that the CTTSO works on require out of the box solutions, which means creative thinking is needed and in many cases, the need to extend the software - which is possible with KNIME. As a result, over 15 custom nodes have been written. This makes the projects easier to manage, because they are still being completed within a single software environment. “We’ve got a lot of great things that help us take the value that KNIME gives us and fit it right in the little square peg, round hole that we’re working in. And we can do it very, very quickly, plus integrate with all the other platforms that the government likes to use” says Brian Frutchey, VANE Technical Lead at BigBear.ai. \n\n This Success Story can be downloaded  here  as a PDF. \n\n \n\n \n"},
{"company": "Allente", "url": "https://www.knime.com/success-story/how-allente-built-recommendation-engine-deliver-personalized-content/", "title": "How Allente built a recommendation engine to deliver personalized content | KNIME", "content": "Open main menu How Allente built a recommendation engine to deliver personalized content Media & Entertainment Marketing Marketing Analytics Providing near real-time recommendations to viewers of streaming content. \n Viewers are increasingly moving away from traditional TV to specialized media providers streaming the exact content they want. Allente offers streaming services, IPTV solutions, and fiber broadband to more than one million customers in Sweden, Norway, Denmark, and Finland.  \n\n While streaming media providers make huge amounts of content choices available to their customers, searching through the library for content they might like to watch can lead to decision fatigue and hamper the viewer experience. In the age of multiple streaming services, this can mean losing a customer to a competitor. To combat that and provide its viewers a better experience, Allente needed a way to deliver customized recommendations to help its viewers reduce the time to find the videos they would be interested in watching. \n\n \n Identifying users' viewing preferences \n Allente knew that to achieve this as fast as possible, they would need a data science platform that offered simplicity of setup and use while delivering the analytic depth to explore data, build models, and deploy them within the same platform. \n\n As an end-to-end data science platform, KNIME perfectly met these requirements. It was an easy decision for Allente to embrace KNIME as its central data science platform to deliver highly relevant, machine learning-powered recommendations to its viewers. \n\n Together with Redfield, a KNIME partner, Allente set out to build a recommendation engine using KNIME. The first step in building the recommendation engine was to connect to Allente’s main data sources in the cloud and on-premises and make sense of the troves of data they held such as event data, video-on-demand catalog, and sample tables from its legacy Oracle database. With KNIME, they built an automated data pipeline that let them connect to these varied data sources, get the data into the right shape, and categorize it into key customer segments easily in one intuitive, visual workflow environment. \n\n Once the key customer segments were defined, Redfield and Allente built and deployed the content recommendation model in the same visual environment offered by KNIME. This involved having the flexibility to code customer segments using Python, optimizing and refining them to improve the accuracy of recommendations, and finally, testing and production, all without switching tools. \n\n The model can predict content that viewers might find engaging, even for new users who do not have a viewing history. Allente is now able to deliver extremely relevant recommendations with the added upshot of improving content distribution efficiency. With KNIME, the team at Allente can easily tune and adapt the model when new consumption data surfaces, ensuring that recommendations remain relevant in the future as well. What’s more, Allente can anticipate customer behavior and predict demand such as which shows might be popular with which customer segments a couple of years from now. \n\n \n Reducing churn and improving LTV \n KNIME’s low-code capabilities have also made machine learning accessible to non-technical teams at Allente. For instance, the customer service team at Allente uses KNIME to predict the likelihood of customers churning and lowers it with targeted retention measures using behavioral insights. They also carry out customer lifetime value analysis to understand return on investment and calculate expected revenue. Additionally, they are able to improve the conversion rate of their upselling activities with personalized offers. \n\n KNIME has accelerated the adoption of a data-driven culture and data-driven decisions at Allente. One of the biggest advantages of using KNIME has been the speed and flexibility of both ETL and spinning up models.  \n\n In the words of an executive at Allente, “For us it has been key to have one tool that could do a little bit of everything. Easy to use – yet advanced and powerful. Having one hub where we can do both data-crunching, machine learning models and automated pipelines in a simple manner has transformed our life.” \n\n This Success Story is available  here  as a PDF. \n"},
{"company": "BGIS", "url": "https://www.knime.com/success-story/how-bgis-saved-400k-annually-automating-work-order-parsing-knime/", "title": "How BGIS saved $400k annually by automating work order parsing with KNIME | KNIME", "content": "Open main menu How BGIS saved $400k annually by automating work order parsing with KNIME Business Services Finance FP&A $420K saved on maintenance and repair costs $35K saved monthly in work order costs \"KNIME has generated a new-found excitement and change in approach towards self-service ETL and data science. This has resulted in significant time savings, improved auditability, and centralization of information via other projects. Workflows are extremely easy for business users to develop in KNIME – making it possible to turnaround client analysis requests in shorter amounts of time, and at levels of granularity and accuracy simply not possible with conventional office software.” Paras Gupta Director, BI & Advanced Analytics \n BGIS manages large capital projects for its clients, some involving a targeted optimization of building systems and equipment to drive efficiency, which aligns well with the company’s sustainability focus. This case focuses on a lighting retrofit conducted in hundreds of a client’s retail sites. Post-retrofit, the client wanted to assess whether the value delivered had been accurately calculated, compare the results with the original business case, and use the outcome of the investigation to inform further retrofit decisions. \n\n Clients accrue several benefits through lighting retrofits – including reductions in electricity consumption, GHG emissions and work order service calls. While electricity and GHG emission reductions are relatively easy to measure or calculate through meter bills, quantifying savings delivered as a result of work order reduction is complicated for several reasons. In this case, work orders represent events when a technician attends a building site to service or replace affected lights. This information consists of numerical as well as text data. To make things more complicated, a lighting retrofit isn’t always about changing to a completely new light bulb. It could be upgrading to a newer model of the same type of light bulb, which was the case here: upgrading from fluorescent tube light type A1 to A2. This is a level of detail simply not categorized in an easy-to-analyze fashion, yet the information is hidden in plain sight - in the text. \n Eliminating the need to manually read through 30,000 work order descriptions \n In order to understand this information, the traditional approach would have been for someone to manually read through work order problem descriptions – 30,000 to be precise – and subjectively categorize them. Furthermore, not all sites participated in the retrofit project, and as the project spanned multiple years, sites were not all retrofit at the same time. The typical approach of analyzing such a large volume of data would be to aggregate information at the client level, which would result in any savings benefits at test sites being diluted by control sites. \n\n In a nutshell: while the data existed in the database, extracting information from large quantities of work orders, and the corresponding textual fields was complex. It simply wasn’t as easy to prove retrofit savings as it may have appeared. The client was relying on a detailed analysis to help inform future lighting benefits, so the analysis had to be spot on to ensure a correct decision. \n Using data science to deep dive into topics and ensure accuracy \n The first step in defining an objective and efficient way to quantify savings was to identify a baseline: when did the retrofit happen, and which sites were retrofit? Looking at the costs on either side of the baseline for test sites, for both retrofit sites and non-retrofit sites, it’s possible to identify the impact of the retrofit. \n\n Proving that the drop was in fact driven by the fluorescent tube light change from type A1 to A2 was the next challenge - as that level of detail is not kept as a tabular record and is difficult to extract. The solution for this was through application of data science. Topic modeling, an unsupervised natural language processing (NLP) technique, was used to read through all work orders’ descriptions and resolutions - to understand what issue occurred, and what work was performed at the site. This technique categorized the service calls in an objective fashion, providing statistics to deep dive into the topics to ensure accuracy. In one case, topic modelling detected a category of work orders (“ceil, height, standard, fluoresce”) where service calls had been initiated to change a fluorescent tube light at ceiling height. This was clearly an activity which was within the scope of the retrofit project objective i.e. the project aimed to reduce such types of work orders. Several other in-scope themes of service calls were also identified which occurred prior to the retrofit. \n\n Topic modelling was conducted in both the pre-retrofit and post-retrofit phases to identify (1) the types and counts of work orders created, (2) whether the underlying issues were those which the retrofit was designed to address, and (3) the associated costs within each type of topic. \n\n The issues that the retrofit was designed to address – like in the example above – reduced dramatically in quantity and in the overall priority post-retrofit. When compared to non-retrofit sites, it became further apparent that the retrofit resulted in cost reductions. Topic modelling allowed BGIS to attribute these savings to the particular type of light bulb that was replaced under the retrofit. \n Results \n At a high level, the lighting retrofit project was budgeted at approximately $4M. The savings were a combination of reductions in both electricity consumption and maintenance and repair costs (M & R work orders). Energy savings were measurable from the bills, while the M & R savings required the topic modeling approach. Therefore the savings just on M & R work orders of $420K (annualized) is substantial as a proportion of the overall project budget and helped to justify the overall project costs. Specific results were: \n\n $420K worth of annual cost savings $35K reduction in average monthly work order costs Potential for even more savings As a service provider, this analysis demonstrated that savings could potentially have been higher had all sites been retrofit – the sites which did not go through the retrofit in fact increased in cost over time over the baseline. Additionally, future retrofits may be condensed over shorter time frames to reap cost avoidance benefits, leading to stronger business cases and savings for clients. From an analysis perspective, modern technology can quickly address common business problems in an objective, transparent, and repeatable fashion. A risk with analyses conducted at aggregate levels is that such analyses can easily provide an incorrect direction (Simpson’s Paradox); in this case, had an aggregated method been used, it could have incorrectly led to an understatement of the savings delivered for the client. \n Why KNIME? \n A review of independent research papers identified KNIME as a Leader in Gartner’s 2018 Magic Quadrant for Data Science and Machine Learning Platforms: a position KNIME had retained for four years prior. Additionally, the total cost of ownership for KNIME was dramatically lower than other software providers in the same quadrant. \n\n As brand new tool to the organization,  KNIME Analytics Platform  was simple to learn thanks to an extensive example library ( KNIME Hub ), several  free and paid online courses , a buzzing online community on  KNIME Forum  (as is typical with many open source tools), and a responsive support team. A key reason for selecting KNIME was the no-fee,  one-click download . Other data science tools were considered, but the high licensing fees quickly made the total cost of ownership unpalatable. Also, because KNIME doesn’t work in competition with existing tools, but rather alongside them, it provided peace of mind that the tools the business is familiar with could still be used – including  Access ,  SQL  and  Tableau . \n\n Getting started with KNIME  was also very easy – thanks to all the free resources available online. Paras Gupta, Director, BI & Advanced Analytics at BGIS went from  \"having zero experience to being an advanced user in under two weeks.” \n\n Furthermore, in this case specifically, the client was able to go back and justify further business cases - helping BGIS to prove value and to continue delivering value to clients. \n\n This Success Story is available  here  as a PDF. \n"},
{"company": "Alexion", "url": "https://www.knime.com/success-story/how-knime-helped-alexion-shorten-time-disease-diagnosis-accelerate-time-treatment/", "title": "How KNIME helped Alexion shorten time to disease diagnosis & accelerate time to treatment | KNIME", "content": "Open main menu How KNIME helped Alexion shorten time to disease diagnosis & accelerate time to treatment Life Sciences (Pharma & Biotech) R&D Diagnostics & Drug Discovery Diagnostic time reduced from several years to a few hours. \n People living with rare diseases often have to wait for years for an accurate diagnosis and many are never diagnosed at all. Even after diagnosis, only 5% of the 7,000 plus known rare diseases have approved therapies. Data plays a key role in diagnosing rare diseases and in the research behind their therapies – in finding insights, experimenting, and discovering new drugs. \n\n Alexion, an AstraZeneca subsidiary, is a global biopharmaceutical company that develops drugs to treat rare disorders. Its goal is to leverage the best available medical data for rare diseases and clinical insights to accelerate the diagnosis of rare conditions. \n\n \n The impact of lacking a central, trusted source of rare disease definitions \n While there is more medical data available than ever before, much of it remains untapped, disorganized, and unusable. For instance, there is no single authority providing a comprehensive list of rare disease definitions. In fact, there isn’t even a consensus on the number of rare diseases that exist. Depending on the source consulted, the answer varies between 7,000 and 9,000 rare conditions. Meanwhile, Alexion’s dataset contains over 12,000 conditions with 7,230 from Orphanet – and many more are in the process of being defined. \n\n Without a trusted and accurate source of rare disease definitions to refer to, it is difficult for physicians to correctly and quickly identify a diagnosis. Most rare diseases lead to considerable disability and early mortality, so a rapid and accurate diagnosis is crucial. Even after receiving a diagnosis, close to 90% of people with a rare disease do not have an approved treatment or therapy. The grim reality is that, of the thousands of rare conditions, only 161 have approved drugs available. The lack of a recognized inventory of rare disease definitions leads to under-representation in healthcare planning and resource allocation and prevents clinical research from being carried out. \n\n What’s more, the lack of a rare disease master dataset also has an impact on medical insurance coverage. The healthcare system in the United States uses ICD-10 codes for billing. There are barely 128 ICD-10 codes for over 7,000 rare conditions, which means that only a minuscule number of rare diseases are recognized by insurance providers and get coverage. \n\n Ultimately, a rare disease definition is the backbone to which all clinically relevant information is mapped, and a lack of it leads to delays in every aspect of disease management from drug discovery, clinical trial design, and drug approval to diagnosis, treatment plan, and insurance coverage. Alexion wanted to address these challenges by building a rare disease data master to shorten the diagnostic journey and time to initiate treatment for those living with rare conditions. \n Ingesting constantly changing medical data \n Alexion adopted KNIME as its core analytics tool to accelerate the rare disease diagnostic odyssey. With KNIME, they were able to easily ingest data from countless biomedical knowledge bases such as  Orphanet ,  PubMed ,  DrugBank ,  Reactome , and  GTex  and get it into the right shape. Before KNIME, if Orphanet or any other data source changed its data format, the team at Alexion would end up spending up to three weeks modifying their Java code, indices, and APIs to accommodate the new format. With KNIME, they were able to seamlessly integrate and transform the data in one uniform, visual environment in no time, regardless of any changes within the data source. \n\n Once the transformed data was loaded, KNIME enabled Alexion to build all-inclusive, master datasets such as the disease master, drug master, gene master, and trial master that helped close major gaps in rare disease information. \n Accurate and actionable rare disease definition dataset \n This data was sent for validation to clinicians and subject matter experts through an interactive dashboard on a KNIME-built data app. These stakeholders - typically PhDs with over 20 years of experience - often insisted on seeing the data lineage and the business logic that was followed for data mapping before they provided feedback. KNIME’s ability to automatically document each step of the data wrangling process enabled Alexion to provide stakeholders the complete visibility they needed into how data pipelines transitioned from raw data all the way to analysis. The feedback from clinicians has led to the creation of a master dataset that is 80% actionable and its quality continues to improve with time. \n Shortened rare disease diagnostic journey \n Today, this rare disease data master serves as an accurate and ready reference tool for physicians to deliver a precise diagnosis quickly. In some cases, the diagnostic time has been brought down to a few hours from several years by matching against the rare disease master. \n\n The rare disease data master also equips physicians with the information they need to provide early and effective treatment for patients after diagnosis. It reduces dependence on unreliable Google searches and addresses the clinical knowledge gap. \n\n Additionally, the rare disease data master forms the center of a flywheel of patient-centric communities around genetics, symptoms, and treatments that work, and it holds the potential to fast-track discovery and approval of new therapies. \n Improved rare disease recognition and medical coverage \n A single, trusted inventory of rare disease data bolsters the chances of higher rare disease representation in healthcare coding systems such as ICD. This would lead to increased interest in clinical research from the healthcare community as well as the much-needed recognition of rare conditions by insurance providers. \n Why KNIME? \n KNIME’s open-source nature was a key factor in it becoming the tool of choice for Alexion. Multiple research institutes that Alexion partners with, were able to start using KNIME without any barriers on scaling. The ability to share complex data with clinicians and subject matter experts through an easy-to-use, interactive data app coupled with the ability to track data lineage was invaluable for Alexion.With KNIME, Alexion and its partners were able to work on the same analytical pipeline while executing it independently on their own datasets. In other words, it helped them bring the analytics to the data and not the other way round. \n\n This Success Story is available  here  as a PDF. \n"}
]